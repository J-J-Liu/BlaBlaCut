import json
import sys
import os
from pathlib import Path

def aggregate_papers_from_list(notes_list, docs_notes_repo_dir):
    """ä»è®ºæ–‡åç§°åˆ—è¡¨ä¸­èšåˆè®ºæ–‡æ•°æ®"""
    aggregated_papers = []
    
    for note_name in notes_list:
        note_folder = docs_notes_repo_dir / note_name
        json_file = note_folder / "info.json"

        if not note_folder.exists():
            print(f"  [è·³è¿‡] æ‰¾ä¸åˆ°ç¬”è®°æ–‡ä»¶å¤¹: {note_name}")
            continue

        if not json_file.exists():
            print(f"  [è·³è¿‡] æ‰¾ä¸åˆ° info.json: {note_name}")
            continue
        
        try:
            with open(json_file, 'r', encoding='utf-8') as jf:
                data = json.load(jf)
                # ç»Ÿä¸€å¤„ç†åˆ—è¡¨æˆ–å­—å…¸ï¼Œå¹¶æ³¨å…¥æ¥æºæ–‡ä»¶å¤¹å
                if isinstance(data, dict):
                    data['_source_folder'] = note_name
                    aggregated_papers.append(data)
                elif isinstance(data, list):
                    for item in data:
                        if isinstance(item, dict):
                            item['_source_folder'] = note_name
                    aggregated_papers.extend(data)
            print(f"  [æˆåŠŸ] å·²åˆå¹¶: {note_name}")
        except Exception as e:
            print(f"  [é”™è¯¯] è¯»å– {note_name} å¤±è´¥: {e}")
    
    return aggregated_papers

def generate_collection_data():
    print("--- MkDocs é›†åˆæ•°æ®ä¸é¡µé¢ç”Ÿæˆå·¥å…· ---")
    
    # 1. ç¡®å®šé…ç½®æ–‡ä»¶è·¯å¾„
    script_dir = Path(__file__).resolve().parent
    
    # é»˜è®¤æ£€æŸ¥è„šæœ¬åŒçº§ç›®å½•ï¼Œæˆ–è€…æ¥å—å‘½ä»¤è¡Œå‚æ•°
    config_path = script_dir / "collection_config.json"
    
    if len(sys.argv) > 1:
        user_arg_path = Path(sys.argv[1])
        # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼ŒåŸºäºå½“å‰å·¥ä½œç›®å½•è§£æ
        config_path = user_arg_path.resolve() if user_arg_path.is_absolute() else (Path.cwd() / user_arg_path).resolve()

    if not config_path.exists():
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶: {config_path}")
        print("ç”¨æ³•: python create_collection_page.py [é…ç½®æ–‡ä»¶è·¯å¾„]")
        return

    print(f"æ­£åœ¨è¯»å–é…ç½®æ–‡ä»¶: {config_path}")

    # 2. è¯»å–ç”¨æˆ·å†™å¥½çš„é…ç½®æ–‡ä»¶
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
    except json.JSONDecodeError:
        print("é”™è¯¯: é…ç½®æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·ç¡®ä¿æ˜¯æ ‡å‡†çš„ JSON æ ¼å¼ã€‚")
        return
    except Exception as e:
        print(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return

    # è·å–é…ç½®é¡¹
    target_dir = config_path.parent
    collection_title = config.get("title")
    collection_description = config.get("description", "") # è·å–æè¿°ï¼Œé»˜è®¤ä¸ºç©º
    
    # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†ç±»ç»“æ„
    has_categories = "categories" in config and isinstance(config.get("categories"), list) and len(config.get("categories", [])) > 0
    notes_list = config.get("papers", [])  # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰åˆ†ç±»ï¼Œä½¿ç”¨ papers

    if not collection_title:
        print("é”™è¯¯: é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘ 'title' (é›†åˆæ ‡é¢˜)ã€‚")
        return
    
    if not has_categories and not notes_list:
        print("è­¦å‘Š: é…ç½®æ–‡ä»¶ä¸­æ—¢æ²¡æœ‰ 'categories' ä¹Ÿæ²¡æœ‰ 'papers' (è®ºæ–‡åˆ—è¡¨) ä¸ºç©ºã€‚")

    # 3. ç¡®å®šæ•°æ®æºç›®å½•
    # å‡è®¾è®ºæ–‡ç¬”è®°ä½äºè„šæœ¬ç›®å½•åŒçº§çš„ ../docs/notes_repo
    docs_notes_repo_dir = (script_dir / "../docs/notes_repo").resolve()

    if not docs_notes_repo_dir.exists():
        print(f"é”™è¯¯: æ— æ³•æ‰¾åˆ°è®ºæ–‡ç¬”è®°æºç›®å½•: {docs_notes_repo_dir}")
        return

    print(f"ç›®æ ‡ä½ç½®: {target_dir}")
    print(f"æ•°æ®æ¥æº: {docs_notes_repo_dir}")

    # 4. èšåˆæ•°æ®
    print(f"\nå¼€å§‹ä¸ºé›†åˆ '{collection_title}' æŸ¥æ‰¾å¹¶åˆå¹¶æ•°æ®...")
    
    if has_categories:
        # æŒ‰åˆ†ç±»å¤„ç†
        categories_data = []
        
        for category in config.get("categories", []):
            cat_title = category.get("title", "æœªå‘½ååˆ†ç±»")
            cat_description = category.get("description", "")
            cat_papers_list = category.get("papers", [])
            
            print(f"\nå¤„ç†åˆ†ç±»: {cat_title}")
            aggregated_papers = aggregate_papers_from_list(cat_papers_list, docs_notes_repo_dir)
            
            categories_data.append({
                "title": cat_title,
                "description": cat_description,
                "papers": aggregated_papers
            })
        
        # æ„å»ºæœ€ç»ˆçš„ JSON æ•°æ®ç»“æ„ï¼ˆå¸¦åˆ†ç±»ï¼‰
        final_output = {
            "title": collection_title,
            "description": collection_description,
            "categories": categories_data
        }
        aggregated_papers = None  # æ ‡è®°ä¸ºåˆ†ç±»æ¨¡å¼
    else:
        # å•å±‚ç»“æ„ï¼ˆå‘åå…¼å®¹ï¼‰
        aggregated_papers = aggregate_papers_from_list(notes_list, docs_notes_repo_dir)
        
        # æ„å»ºæœ€ç»ˆçš„ JSON æ•°æ®ç»“æ„ï¼ˆå•å±‚ï¼‰
        final_output = {
            "title": collection_title,
            "description": collection_description,
            "papers": aggregated_papers
        }
        categories_data = None  # æ ‡è®°ä¸ºå•å±‚æ¨¡å¼

    # 6. ä¿å­˜ JSON ç»“æœ (æ–‡ä»¶åæ”¹ä¸º collected_info.json)
    output_json_path = target_dir / "collected_info.json"
    try:
        with open(output_json_path, 'w', encoding='utf-8') as out_f:
            json.dump(final_output, out_f, indent=4, ensure_ascii=False)
        print(f"\n[1/2] JSON æ•°æ®å·²ä¿å­˜è‡³: {output_json_path}")
    except Exception as e:
        print(f"ä¿å­˜ JSON æ–‡ä»¶å¤±è´¥: {e}")

    # 7. ç”Ÿæˆ MkDocs Markdown æ–‡ä»¶ (æ–‡ä»¶åæ”¹ä¸º index.md)
    try:
        output_md_path = target_dir / "index.md"

        # è®¡ç®—ä»ç›®æ ‡ç›®å½•(Markdownæ‰€åœ¨ç›®å½•)åˆ°è®ºæ–‡ç¬”è®°ç›®å½•çš„ç›¸å¯¹è·¯å¾„
        relative_path_to_notes = os.path.relpath(docs_notes_repo_dir, target_dir)
        relative_path_to_notes = Path(relative_path_to_notes).as_posix()

        md_content = []
        md_content.append(f"# {collection_title}")
        md_content.append("")
        
        # å¦‚æœæœ‰æè¿°ï¼Œæ˜¾ç¤ºæè¿°
        if collection_description:
            md_content.append(collection_description)
            md_content.append("")

        # ç”Ÿæˆè®ºæ–‡æ¡ç›®çš„è¾…åŠ©å‡½æ•°
        def generate_paper_entry(paper, md_content, heading_level=2):
            # æå–å­—æ®µ
            p_title = paper.get('paper_title', paper.get('title', 'Unknown Title'))
            desc = paper.get('description', 'æš‚æ— æè¿°')
            source_folder = paper.get('_source_folder', '')
            
            # æå– metadata
            meta = paper.get('metadata', {})
            
            # é²æ£’æ€§å¤„ç†ï¼šAuthors
            authors_raw = meta.get('authors', [])
            if isinstance(authors_raw, list):
                authors = ", ".join([str(x) for x in authors_raw if x])
            else:
                authors = str(authors_raw) if authors_raw else "Unknown Authors"

            # é²æ£’æ€§å¤„ç†ï¼šAffiliations
            affiliations_raw = meta.get('affiliations', [])
            if isinstance(affiliations_raw, list):
                affiliations = ", ".join([str(x) for x in affiliations_raw if x])
            else:
                affiliations = str(affiliations_raw) if affiliations_raw else ""
                
            # é²æ£’æ€§å¤„ç†ï¼šVenue & Year
            venue = str(meta.get('venue', 'Unknown Venue')).strip()
            year = str(meta.get('year', '')).strip()
            
            # æå–å’Œå¤„ç† DOI
            doi_raw = meta.get('doi')
            doi = None
            if doi_raw and str(doi_raw).strip().lower() not in ['null', 'none', '']:
                doi_str = str(doi_raw).strip()
                # å¦‚æœ DOI åŒ…å«å®Œæ•´ URLï¼Œæå– DOI éƒ¨åˆ†
                if doi_str.startswith('http://doi.org/') or doi_str.startswith('https://doi.org/'):
                    doi = doi_str.split('doi.org/')[-1]
                elif doi_str.startswith('doi:'):
                    doi = doi_str.replace('doi:', '').strip()
                else:
                    doi = doi_str
            
            # æ„å»ºè·¯å¾„
            base_link = f"{relative_path_to_notes}/{source_folder}"

            # æ„å»º Markdown å†…å®¹å—ï¼ˆæ ¹æ® heading_level åŠ¨æ€ç”Ÿæˆæ ‡é¢˜çº§åˆ«ï¼‰
            heading_prefix = "#" * heading_level
            md_content.append(f"{heading_prefix} {p_title}")
            md_content.append(f"> **Authors:** {authors}  ")
            
            if affiliations:
                md_content.append(f"> **Affiliations:** {affiliations}  ")
                
            md_content.append(f"> **Venue:** {venue} {year}")
            md_content.append("")
            md_content.append(f"{desc}")
            md_content.append("")
            
            links = []
            links.append(f"[ğŸ“„ è®ºæ–‡ç¬”è®°]({base_link}/paper_notes.md)")
            links.append(f"[ğŸ“Š å›¾è¡¨è§£æ]({base_link}/figs_notes.md)")
            links.append(f"[ğŸ‘¶ ELI5 è§£é‡Š]({base_link}/ELI5_notes.md)")
            
            # å¦‚æœ DOI æœ‰æ•ˆï¼Œæ·»åŠ ç›´è¾¾åŸæ–‡é“¾æ¥ï¼ˆä½¿ç”¨ HTML æ ¼å¼ä»¥åœ¨æ–°æ ‡ç­¾é¡µæ‰“å¼€ï¼‰
            if doi:
                links.append(f'<a href="https://doi.org/{doi}" target="_blank" rel="noopener noreferrer">ğŸ”— ç›´è¾¾åŸæ–‡</a>')
            
            md_content.append(" | ".join(links))
            md_content.append("")
            md_content.append("---")
            md_content.append("")

        if has_categories:
            # æŒ‰åˆ†ç±»ç»„ç»‡æ˜¾ç¤º
            total_papers_count = sum(len(cat.get("papers", [])) for cat in categories_data)
            md_content.append(f"æœ¬é¡µé¢å…±æ”¶å½•äº† {total_papers_count} ç¯‡è®ºæ–‡ç¬”è®°ï¼Œåˆ†ä¸º {len(categories_data)} ä¸ªåˆ†ç±»ã€‚")
            md_content.append("")
            
            for category in categories_data:
                cat_title = category.get("title", "æœªå‘½ååˆ†ç±»")
                cat_description = category.get("description", "")
                cat_papers = category.get("papers", [])
                
                md_content.append(f"## {cat_title}")
                md_content.append("")
                
                if cat_description:
                    md_content.append(cat_description)
                    md_content.append("")
                
                md_content.append(f"*æœ¬åˆ†ç±»åŒ…å« {len(cat_papers)} ç¯‡è®ºæ–‡*")
                md_content.append("")
                
                for paper in cat_papers:
                    generate_paper_entry(paper, md_content, heading_level=3)
        else:
            # å•å±‚ç»“æ„æ˜¾ç¤ºï¼ˆå‘åå…¼å®¹ï¼‰
            md_content.append(f"æœ¬é¡µé¢å…±æ”¶å½•äº† {len(aggregated_papers)} ç¯‡è®ºæ–‡ç¬”è®°ã€‚")
            md_content.append("")
            
            for paper in aggregated_papers:
                generate_paper_entry(paper, md_content)

        with open(output_md_path, 'w', encoding='utf-8') as f:
            f.write("\n".join(md_content))
        
        if has_categories:
            total_count = sum(len(cat.get("papers", [])) for cat in categories_data)
            print(f"[2/2] Markdown é¡µé¢å·²ç”Ÿæˆè‡³: {output_md_path}")
            print(f"\nå¤„ç†å®Œæˆï¼å…±åŒ…å« {len(categories_data)} ä¸ªåˆ†ç±»ï¼Œ{total_count} ç¯‡è®ºæ–‡ã€‚")
        else:
            print(f"[2/2] Markdown é¡µé¢å·²ç”Ÿæˆè‡³: {output_md_path}")
            print(f"\nå¤„ç†å®Œæˆï¼å…±åŒ…å« {len(aggregated_papers)} ç¯‡è®ºæ–‡ã€‚")
        
    except Exception as e:
        print(f"ç”Ÿæˆ Markdown æ–‡ä»¶å¤±è´¥: {e}")

if __name__ == "__main__":
    generate_collection_data()